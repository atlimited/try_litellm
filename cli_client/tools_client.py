import requests
import json
import os
import argparse

## mitmproxy ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã€ç’°å¢ƒå¤‰æ•°ã§ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’è¡Œã„ã¾ã™
os.environ["HTTP_PROXY"] = "http://127.0.0.1:8080"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:8080"

# mitmproxy ã®è¨¼æ˜æ›¸ã‚’è¨­å®š
ca_cert_path = "/Users/takagi/.mitmproxy/mitmproxy-ca-cert.pem"
os.environ['SSL_CERT_FILE'] = ca_cert_path
os.environ['REQUESTS_CA_BUNDLE'] = ca_cert_path

# LiteLLMãƒ—ãƒ­ã‚­ã‚·ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
LITELLM_PROXY_URL = "http://localhost:4000/v1/chat/completions"
HEADERS = {
    "Content-Type": "application/json",
}

# ã‚µãƒ³ãƒ—ãƒ«é–¢æ•°: æŒ‡å®šã—ãŸå ´æ‰€ã®å¤©æ°—ã‚’è¿”ã™
def get_current_weather(location, unit="fahrenheit"):
    """Get the current weather in a given location"""
    if "tokyo" in location.lower():
        return json.dumps({"location": "Tokyo", "temperature": "10", "unit": "celsius"})
    elif "san francisco" in location.lower():
        return json.dumps({"location": "San Francisco", "temperature": "72", "unit": "fahrenheit"})
    elif "paris" in location.lower():
        return json.dumps({"location": "Paris", "temperature": "22", "unit": "celsius"})
    else:
        return json.dumps({"location": location, "temperature": "unknown"})

def run_tool_call(message, model):
    """Function callingã‚’ä½¿ã£ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™"""

    # ãƒ„ãƒ¼ãƒ«å®šç¾©
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Get the current weather in a given location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                    },
                    "required": ["location"],
                },
            },
        }
    ]

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æº–å‚™
    messages = [{"role": "user", "content": message}]

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
    payload = {
        "model": model,
        "messages": messages,
        "tools": tools,
        "tool_choice": "auto",
    }

    print(f"ğŸš€ {model}ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
    try:
        response = requests.post(LITELLM_PROXY_URL, headers=HEADERS, json=payload)
        response.raise_for_status()
        result = response.json()

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
        print("\nğŸ¤– LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹:\n")
        response_message = result["choices"][0]["message"]

        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹ã‹ç¢ºèª
        if "tool_calls" in response_message:
            tool_calls = response_message["tool_calls"]
            print(f"ğŸ”§ ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {len(tool_calls)}å€‹")

            # å„ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å‡¦ç†
            for tool_call in tool_calls:
                function_name = tool_call["function"]["name"]
                function_args = json.loads(tool_call["function"]["arguments"])

                print(f"\nğŸ“ é–¢æ•° '{function_name}' ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚å¼•æ•°: {function_args}")

                # é–¢æ•°ã‚’å®Ÿè¡Œ
                if function_name == "get_current_weather":
                    function_response = get_current_weather(
                        location=function_args.get("location", ""),
                        unit=function_args.get("unit", "fahrenheit")
                    )

                    print(f"ğŸŒ¤ å¤©æ°—æƒ…å ±: {function_response}")

                    # é–¢æ•°ã®çµæœã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ 
                    messages.append(response_message)
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call["id"],
                        "name": function_name,
                        "content": function_response,
                    })

            # é–¢æ•°çµæœã‚’å«ã‚ã¦å†åº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            second_payload = {
                "model": model,
                "messages": messages,
                "tools": tools, # anthropicã§ã¯2å›ç›®ã‚‚å¿…è¦
                "tool_choice": "auto", # anthropicã§ã¯2å›ç›®ã‚‚å¿…è¦
            }

            print("\nğŸ”„ é–¢æ•°ã®çµæœã‚’å«ã‚ã¦å†åº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")

            second_response = requests.post(LITELLM_PROXY_URL, headers=HEADERS, json=second_payload)
            second_response.raise_for_status()
            second_result = second_response.json()

            print("\nğŸ¤– æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹:\n")
            final_message = second_result["choices"][0]["message"]["content"]
            print(final_message)
            return final_message
        else:
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒãªã„å ´åˆã¯ç›´æ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            content = response_message["content"]
            print(content)
            return content

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã¦å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="Function Callingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: OpenAIã®Function Callingã‚’ä½¿ã£ãŸãƒ‡ãƒ¢")
    parser.add_argument("message", help="LLMã«é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    parser.add_argument("-m", "--model", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«", default="OpenAI/gpt-4o-mini")

    args = parser.parse_args()
    run_tool_call(args.message, args.model)

if __name__ == "__main__":
    main()
