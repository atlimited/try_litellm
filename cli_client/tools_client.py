#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Function Callingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ (çµ±åˆç‰ˆ)
OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¾ãŸã¯requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦LiteLLM Proxyã®Function Callingæ©Ÿèƒ½ã‚’å‘¼ã³å‡ºã™
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆopenai/requestsï¼‰ã‚’é¸æŠã§ãã¾ã™
"""

import os
import sys
import json
import argparse
import requests
from typing import Optional, Dict, Any, Union, List

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (optional)
try:
    from openai import OpenAI
    OPENAI_CLIENT_AVAILABLE = True
except ImportError:
    OPENAI_CLIENT_AVAILABLE = False
    print("OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚requestsãƒ¢ãƒ¼ãƒ‰ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚")

# LiteLLM Proxy APIã®ãƒ™ãƒ¼ã‚¹URL
BASE_URL = "http://0.0.0.0:4000/v1"

# APIã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹ã‹ã€ç©ºæ–‡å­—åˆ—ã‚’ä½¿ç”¨ï¼‰
API_KEY = os.environ.get("OPENAI_API_KEY", "")

# ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã™ï¼‰
#model_name = "OpenAI/gpt-4o-mini"
#model_name = "Google/gemini-2.0-flash"
model_name = "SambaNova/Meta-Llama-3.3-70B-Instruct"
#model_name = "SambaNova/Llama-4-Maverick-17B-128E-Instruct"

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
openai_client = None
if OPENAI_CLIENT_AVAILABLE:
    openai_client = OpenAI(
        base_url=BASE_URL,
        api_key=API_KEY
    )

## ã‚µãƒ³ãƒ—ãƒ«é–¢æ•°: æŒ‡å®šã—ãŸå ´æ‰€ã®å¤©æ°—ã‚’è¿”ã™
def get_current_weather(location, unit="fahrenheit"):
    """Get the current weather in a given location"""
    if "tokyo" in location.lower():
        location = "Tokyo"
        temperature = "10"
        unit = "celsius"
    elif "san francisco" in location.lower():
        location = "San Francisco"
        temperature = "72"
        unit = "fahrenheit"
    elif "paris" in location.lower():
        location = "Paris"
        temperature = "22"
        unit = "celsius"
    else:
        temperature = "undefined"
    return json.dumps({"location": location, "temperature": temperature, "unit": unit})

def get_tools_definition():
    """ãƒ„ãƒ¼ãƒ«å®šç¾©ã‚’è¿”ã™"""
    return [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Get the current weather in a given location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                    },
                    "required": ["location"],
                },
            },
        }
    ]

def execute_function_call(function_name, function_args):
    """é–¢æ•°åã¨å¼•æ•°ã‹ã‚‰é©åˆ‡ãªé–¢æ•°ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™"""
    if function_name == "get_current_weather":
        return get_current_weather(
            location=function_args.get("location", ""),
            unit=function_args.get("unit", "fahrenheit")
        )
    else:
        return json.dumps({"error": f"Unknown function: {function_name}"})

def run_tool_call_with_openai(message: str, model: str = model_name) -> str:
    """
    OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦Function Callingãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
    
    Args:
        message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    if not OPENAI_CLIENT_AVAILABLE or openai_client is None:
        print("âŒ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚requestsãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
        return run_tool_call_with_requests(message, model)
    
    try:
        # ãƒ„ãƒ¼ãƒ«å®šç¾©
        tools = get_tools_definition()
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æº–å‚™
        messages = [{"role": "user", "content": message}]
        
        # æœ€åˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        print(f"ğŸš€ {model}ã«OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")
        response = openai_client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
        print("\nğŸ¤– LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹:\n")
        response_message = response.choices[0].message
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹ã‹ç¢ºèª
        if hasattr(response_message, 'tool_calls') and response_message.tool_calls:
            tool_calls = response_message.tool_calls
            print(f"ğŸ”§ ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {len(tool_calls)}å€‹")
            
            # å„ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å‡¦ç†
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                print(f"\nğŸ“ é–¢æ•° '{function_name}' ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚å¼•æ•°: {function_args}")
                
                # é–¢æ•°ã‚’å®Ÿè¡Œ
                function_response = execute_function_call(function_name, function_args)
                
                print(f"ğŸŒ¤ é–¢æ•°ã®çµæœ: {function_response}")
                
                # é–¢æ•°ã®çµæœã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ 
                messages.append({
                    "role": "assistant",
                    "tool_calls": [
                        {
                            "id": tool_call.id,
                            "type": "function",
                            "function": {
                                "name": function_name,
                                "arguments": tool_call.function.arguments
                            }
                        }
                    ]
                })
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": function_name,
                    "content": function_response,
                })
            
            # é–¢æ•°çµæœã‚’å«ã‚ã¦å†åº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            print("\nğŸ”„ é–¢æ•°ã®çµæœã‚’å«ã‚ã¦å†åº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")
            
            second_response = openai_client.chat.completions.create(
                model=model,
                messages=messages,
                tools=tools,  # anthropicã§ã¯2å›ç›®ã‚‚å¿…è¦
                tool_choice="auto"  # anthropicã§ã¯2å›ç›®ã‚‚å¿…è¦
            )
            
            print("\nğŸ¤– æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹:\n")
            final_message = second_response.choices[0].message.content
            print(final_message)
            return final_message
        else:
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒãªã„å ´åˆã¯ç›´æ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            content = response_message.content
            print(content)
            return content
            
    except Exception as e:
        print(f"âŒ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("â†ªï¸ requestsãƒ¢ãƒ¼ãƒ‰ã§å†è©¦è¡Œã—ã¾ã™")
        return run_tool_call_with_requests(message, model)

def run_tool_call_with_requests(message: str, model: str = model_name) -> str:
    """
    requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦Function Callingãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
    
    Args:
        message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        endpoint = f"{BASE_URL}/chat/completions"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = {
            "Content-Type": "application/json"
        }
        
        # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ 
        if API_KEY:
            headers["Authorization"] = f"Bearer {API_KEY}"
        
        # ãƒ„ãƒ¼ãƒ«å®šç¾©
        tools = get_tools_definition()
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æº–å‚™
        messages = [{"role": "user", "content": message}]
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
        payload = {
            "model": model,
            "messages": messages,
            "tools": tools,
            "tool_choice": "auto",
        }
        
        print(f"ğŸš€ {model}ã«requestsã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
        response = requests.post(endpoint, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
        print("\nğŸ¤– LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹:\n")
        response_message = result["choices"][0]["message"]
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹ã‹ç¢ºèª
        if "tool_calls" in response_message:
            tool_calls = response_message["tool_calls"]
            print(f"ğŸ”§ ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {len(tool_calls)}å€‹")
            
            # å„ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å‡¦ç†
            for tool_call in tool_calls:
                function_name = tool_call["function"]["name"]
                function_args = json.loads(tool_call["function"]["arguments"])
                
                print(f"\nğŸ“ é–¢æ•° '{function_name}' ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚å¼•æ•°: {function_args}")
                
                # é–¢æ•°ã‚’å®Ÿè¡Œ
                function_response = execute_function_call(function_name, function_args)
                
                print(f"ğŸŒ¤ é–¢æ•°ã®çµæœ: {function_response}")
                
                # é–¢æ•°ã®çµæœã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ 
                messages.append(response_message)
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "name": function_name,
                    "content": function_response,
                })
            
            # é–¢æ•°çµæœã‚’å«ã‚ã¦å†åº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            second_payload = {
                "model": model,
                "messages": messages,
                "tools": tools,  # anthropicã§ã¯2å›ç›®ã‚‚å¿…è¦
                "tool_choice": "auto",  # anthropicã§ã¯2å›ç›®ã‚‚å¿…è¦
            }
            
            print("\nğŸ”„ é–¢æ•°ã®çµæœã‚’å«ã‚ã¦å†åº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")
            
            second_response = requests.post(endpoint, headers=headers, json=second_payload)
            second_response.raise_for_status()
            second_result = second_response.json()
            
            print("\nğŸ¤– æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹:\n")
            final_message = second_result["choices"][0]["message"]["content"]
            print(final_message)
            return final_message
        else:
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒãªã„å ´åˆã¯ç›´æ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            content = response_message["content"]
            print(content)
            return content
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}")
        return ""

def run_tool_call(message: str, model: str = model_name, client_type: str = "auto") -> str:
    """
    Function Callingãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ï¼ˆçµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰
    
    Args:
        message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        client_type: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆopenai/requests/autoï¼‰
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    print(f"ğŸ“ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model}")
    print(f"ğŸ”§ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: {client_type}")
    
    if client_type == "openai" and OPENAI_CLIENT_AVAILABLE:
        return run_tool_call_with_openai(message, model)
    elif client_type == "requests" or not OPENAI_CLIENT_AVAILABLE:
        return run_tool_call_with_requests(message, model)
    else:  # auto
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ãã‚Œã‚’ä½¿ç”¨ã€ãã†ã§ãªã‘ã‚Œã°requests
        if OPENAI_CLIENT_AVAILABLE:
            return run_tool_call_with_openai(message, model)
        else:
            return run_tool_call_with_requests(message, model)

def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼šã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã¦æ©Ÿèƒ½ã‚’å®Ÿè¡Œ
    """
    parser = argparse.ArgumentParser(description="çµ±åˆFunction Callingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ")
    parser.add_argument("message", help="LLMã«é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    parser.add_argument("--model", "-m", default=model_name, help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--client", "-c", choices=["openai", "requests", "auto"], default="auto",
                      help="ä½¿ç”¨ã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆopenai/requests/autoï¼‰")
    
    args = parser.parse_args()
    
    run_tool_call(args.message, args.model, args.client)

if __name__ == "__main__":
    main()
