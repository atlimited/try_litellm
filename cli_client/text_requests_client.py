#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦LiteLLM Proxyã®OpenAIäº’æ›APIã‚’å‘¼ã³å‡ºã™
"""

import requests
import json
import os
import sys

# LiteLLM Proxy APIã®ãƒ™ãƒ¼ã‚¹URL
BASE_URL = "http://0.0.0.0:4000/v1"

# APIã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹ã‹ã€ç©ºæ–‡å­—åˆ—ã‚’ä½¿ç”¨ï¼‰
API_KEY = os.environ.get("OPENAI_API_KEY", "")

# ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã™ï¼‰
#model_name = "OpenAI/gpt-4o-mini"
#model_name = "Google/gemini-2.0-flash"
model_name = "SambaNova/Meta-Llama-3.2-3B-Instruct"
#model_name = "SambaNova/Llama-4-Maverick-17B-128E-Instruct"

def generate_text(prompt, model=model_name):
    """
    ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
    
    Args:
        prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model}")
    print("ğŸ”„ å¿œç­”ã‚’ç”Ÿæˆä¸­...")
    
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    endpoint = f"{BASE_URL}/chat/completions"
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    headers = {
        "Content-Type": "application/json"
    }
    
    # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ 
    if API_KEY:
        headers["Authorization"] = f"Bearer {API_KEY}"
    
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæœ¬æ–‡
    payload = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ]
    }
    
    try:
        # APIå‘¼ã³å‡ºã—
        response = requests.post(endpoint, headers=headers, json=payload)
        response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿ
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
        result = response.json()
        
        # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’æŠ½å‡º
        if "choices" in result and len(result["choices"]) > 0:
            message = result["choices"][0]["message"]
            if "content" in message:
                text_response = message["content"]
                print(f"\nğŸ“ å›ç­”:\n{text_response}")
                return text_response
        
        print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆå›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {result}")
        return ""
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}")
        return ""

def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼šã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã¦æ©Ÿèƒ½ã‚’å®Ÿè¡Œ
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='requestsã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ')
    parser.add_argument('message', help='ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ')
    parser.add_argument('--model', '-m', default=model_name, help='ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å')
    
    args = parser.parse_args()
    
    generate_text(args.message, args.model)

if __name__ == "__main__":
    main()
