#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ (çµ±åˆç‰ˆ)
OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¾ãŸã¯requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦LiteLLM Proxyã®OpenAIäº’æ›APIã‚’å‘¼ã³å‡ºã™
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆopenai/requestsï¼‰ã‚’é¸æŠã§ãã¾ã™
"""

import os
import sys
import json
import base64
import argparse
import requests
from PIL import Image
import io
import re
from pathlib import Path
from typing import Optional, Dict, Any, Union, List

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (optional)
try:
    from openai import OpenAI
    OPENAI_CLIENT_AVAILABLE = True
except ImportError:
    OPENAI_CLIENT_AVAILABLE = False
    print("OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚requestsãƒ¢ãƒ¼ãƒ‰ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚")

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")

# LiteLLM Proxy APIã®ãƒ™ãƒ¼ã‚¹URL
BASE_URL = "http://0.0.0.0:4000/v1"

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
openai_client = None
if OPENAI_CLIENT_AVAILABLE:
    openai_client = OpenAI(
        base_url=BASE_URL,
        api_key=GEMINI_API_KEY
    )

def chat_with_openai(prompt: str, model: str = "Google/gemini-2.0-flash") -> str:
    """
    OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦AIãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ£ãƒƒãƒˆã™ã‚‹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰
    
    Args:
        prompt: ãƒãƒ£ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå›ç­”
    """
    try:
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        response = openai_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        if response.choices and len(response.choices) > 0:
            text_response = response.choices[0].message.content
            print("\nğŸ“ å›ç­”:")
            print(text_response)
            return text_response
        else:
            print("âŒ å¿œç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return ""
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return ""

def chat_with_requests(prompt: str, model: str = "Google/gemini-2.0-flash") -> str:
    """
    requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦AIãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ£ãƒƒãƒˆã™ã‚‹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰
    
    Args:
        prompt: ãƒãƒ£ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå›ç­”
    """
    try:
        # LiteLLMãƒ—ãƒ­ã‚­ã‚·ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        url = f"{BASE_URL}/chat/completions"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = {
            "Content-Type": "application/json"
        }
        
        # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ 
        if GEMINI_API_KEY:
            headers["Authorization"] = f"Bearer {GEMINI_API_KEY}"
        
        # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user", 
                    "content": prompt
                }
            ]
        }
        
        # LiteLLMãƒ—ãƒ­ã‚­ã‚·APIã‚’å‘¼ã³å‡ºã™
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
        result = response.json()
        
        # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’æŠ½å‡º
        if "choices" in result and len(result["choices"]) > 0:
            text_response = result["choices"][0]["message"]["content"]
            print("\nğŸ“ å›ç­”:")
            print(text_response)
            return text_response
        else:
            print("âŒ å¿œç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return ""
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}")
        return ""

def chat(prompt: str, model: str = "Google/gemini-2.0-flash", client_type: str = "auto") -> str:
    """
    AIãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ£ãƒƒãƒˆã‚’ã™ã‚‹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰- çµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    
    Args:
        prompt: ãƒãƒ£ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        client_type: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆopenai/requests/autoï¼‰
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå›ç­”
    """
    print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model}")
    print(f"ğŸ”§ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: {client_type}")
    print("ğŸ”„ å¿œç­”ã‚’ç”Ÿæˆä¸­...")
    
    if client_type == "openai" and OPENAI_CLIENT_AVAILABLE:
        return chat_with_openai(prompt, model)
    elif client_type == "requests" or not OPENAI_CLIENT_AVAILABLE:
        return chat_with_requests(prompt, model)
    else:  # auto
        try:
            # ã¾ãšrequestsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è©¦ã™
            return chat_with_requests(prompt, model)
        except Exception as e:
            print(f"âŒ requestsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            print("â†ªï¸ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§å†è©¦è¡Œã—ã¾ã™")
            
            # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if OPENAI_CLIENT_AVAILABLE:
                return chat_with_openai(prompt, model)
            else:
                print("âŒ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return ""

def encode_image_to_base64(image_path: str) -> str:
    """
    ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹
    
    Args:
        image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒãƒ‡ãƒ¼ã‚¿
    """
    try:
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
        with open(image_path, "rb") as image_file:
            # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
            image_data = image_file.read()
            
            # ç”»åƒå½¢å¼ã‚’æ¤œå‡º
            try:
                image = Image.open(io.BytesIO(image_data))
                image_format = image.format.lower()
            except Exception:
                # ç”»åƒå½¢å¼ã‚’æ¤œå‡ºã§ããªã„å ´åˆã¯jpegã¨ä»®å®š
                image_format = "jpeg"
            
            # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            base64_data = base64.b64encode(image_data).decode("utf-8")
            
            # æ­£ã—ã„MIMEã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            return f"data:image/{image_format};base64,{base64_data}"
            
    except Exception as e:
        print(f"âŒ ç”»åƒã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return ""

def analyze_image_with_openai(image_path: str, prompt: str = "ã“ã‚Œã¯ãªã‚“ã®ç”»åƒã§ã™ã‹", model: str = "Google/gemini-2.0-flash") -> str:
    """
    OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’åˆ†æã™ã‚‹
    
    Args:
        image_path: åˆ†æã™ã‚‹ç”»åƒã®ãƒ‘ã‚¹
        prompt: ç”»åƒã«é–¢ã™ã‚‹è³ªå•ã‚„æŒ‡ç¤º
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå›ç­”
    """
    try:
        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        base64_image = encode_image_to_base64(image_path)
        
        if not base64_image:
            print("âŒ ç”»åƒã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return ""
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        response = openai_client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": base64_image}
                        }
                    ]
                }
            ]
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        if response.choices and len(response.choices) > 0:
            text_response = response.choices[0].message.content
            print("\nğŸ“ å›ç­”:")
            print(text_response)
            return text_response
        else:
            print("âŒ å¿œç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return ""
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return ""

def analyze_image_with_requests(image_path: str, prompt: str = "ã“ã‚Œã¯ãªã‚“ã®ç”»åƒã§ã™ã‹", model: str = "Google/gemini-2.0-flash") -> str:
    """
    requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’åˆ†æã™ã‚‹
    
    Args:
        image_path: åˆ†æã™ã‚‹ç”»åƒã®ãƒ‘ã‚¹
        prompt: ç”»åƒã«é–¢ã™ã‚‹è³ªå•ã‚„æŒ‡ç¤º
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå›ç­”
    """
    try:
        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        base64_image = encode_image_to_base64(image_path)
        
        if not base64_image:
            print("âŒ ç”»åƒã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return ""
        
        # LiteLLMãƒ—ãƒ­ã‚­ã‚·ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        url = f"{BASE_URL}/chat/completions"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = {
            "Content-Type": "application/json"
        }
        
        # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ 
        if GEMINI_API_KEY:
            headers["Authorization"] = f"Bearer {GEMINI_API_KEY}"
        
        # Geminiã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ç”¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": base64_image
                            }
                        }
                    ]
                }
            ],
            "modalities": ["image", "text"]  # ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        }
        
        try:
            # LiteLLMãƒ—ãƒ­ã‚­ã‚·APIã‚’å‘¼ã³å‡ºã™
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = response.json()
            
            # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’æŠ½å‡º
            if "choices" in result and len(result["choices"]) > 0:
                text_response = result["choices"][0]["message"]["content"]
                print("\nğŸ“ å›ç­”:")
                print(text_response)
                return text_response
            else:
                print("âŒ å¿œç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return ""
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            if hasattr(e, 'response') and hasattr(e.response, 'text'):
                print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}")
            return ""
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return ""

def analyze_image(image_path: str, prompt: str = "ã“ã‚Œã¯ãªã‚“ã®ç”»åƒã§ã™ã‹", model: str = "Google/gemini-2.0-flash", client_type: str = "auto") -> str:
    """
    ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ - çµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    
    Args:
        image_path: åˆ†æã™ã‚‹ç”»åƒã®ãƒ‘ã‚¹
        prompt: ç”»åƒã«é–¢ã™ã‚‹è³ªå•ã‚„æŒ‡ç¤º
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        client_type: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆopenai/requests/autoï¼‰
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå›ç­”
    """
    print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
    print(f"ğŸ–¼ï¸ ç”»åƒ: {image_path}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model}")
    print(f"ğŸ”§ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: {client_type}")
    print("ğŸ”„ ç”»åƒã‚’åˆ†æä¸­...")
    
    if client_type == "openai" and OPENAI_CLIENT_AVAILABLE:
        return analyze_image_with_openai(image_path, prompt, model)
    elif client_type == "requests" or not OPENAI_CLIENT_AVAILABLE:
        return analyze_image_with_requests(image_path, prompt, model)
    else:  # auto
        try:
            # ã¾ãšrequestsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è©¦ã™
            return analyze_image_with_requests(image_path, prompt, model)
        except Exception as e:
            print(f"âŒ requestsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            print("â†ªï¸ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§å†è©¦è¡Œã—ã¾ã™")
            
            # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if OPENAI_CLIENT_AVAILABLE:
                return analyze_image_with_openai(image_path, prompt, model)
            else:
                print("âŒ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return ""

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã¦æ©Ÿèƒ½ã‚’å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description='Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ')
    subparsers = parser.add_subparsers(dest='command', help='ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰')
    
    # ãƒãƒ£ãƒƒãƒˆã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰
    chat_parser = subparsers.add_parser('chat', help='ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒƒãƒˆ')
    chat_parser.add_argument('prompt', help='ãƒãƒ£ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ')
    chat_parser.add_argument('--model', default="Google/gemini-2.0-flash", help='ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å')
    chat_parser.add_argument('--client', choices=['openai', 'requests', 'auto'], default='auto', 
                             help='ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ— (openai/requests/auto)')
    
    # ç”»åƒèªè­˜ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰
    vision_parser = subparsers.add_parser('vision', help='ç”»åƒèªè­˜')
    vision_parser.add_argument('prompt', help='ç”»åƒã«é–¢ã™ã‚‹è³ªå•ã‚„æŒ‡ç¤º')
    vision_parser.add_argument('image', help='åˆ†æã™ã‚‹ç”»åƒã®ãƒ‘ã‚¹')
    vision_parser.add_argument('--model', default="Google/gemini-2.0-flash", help='ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å')
    vision_parser.add_argument('--client', choices=['openai', 'requests', 'auto'], default='auto', 
                             help='ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ— (openai/requests/auto)')
    
    args = parser.parse_args()
    
    if args.command == 'chat':
        chat(args.prompt, args.model, args.client)
    elif args.command == 'vision':
        analyze_image(args.image, args.prompt, args.model, args.client)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
