#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éŸ³å£°å‡¦ç†ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ (çµ±åˆç‰ˆ)
OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¾ãŸã¯requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦LiteLLM Proxyã®OpenAIäº’æ›APIã‚’å‘¼ã³å‡ºã™
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆopenai/requestsï¼‰ã‚’é¸æŠã§ãã¾ã™
"""

import os
import sys
import json
import argparse
import requests
import tempfile
import base64
from typing import Optional, Dict, Any, Union, List, Tuple

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (optional)
try:
    from openai import OpenAI
    OPENAI_CLIENT_AVAILABLE = True
except ImportError:
    OPENAI_CLIENT_AVAILABLE = False
    print("OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚requestsãƒ¢ãƒ¼ãƒ‰ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚")

# LiteLLM Proxy APIã®ãƒ™ãƒ¼ã‚¹URL
BASE_URL = "http://0.0.0.0:4000/v1"

# APIã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹ã‹ã€ç©ºæ–‡å­—åˆ—ã‚’ä½¿ç”¨ï¼‰
API_KEY = os.environ.get("OPENAI_API_KEY", "")

# ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã™ï¼‰
#model_name = "gpt-4o-audio-preview"
model_name = "SambaNova/Whisper-Large-v3"
#model_name = "SambaNova/Qwen2-Audio-7B-Instruct"
#model_name = 'OpenAI/whisper-1'
#model_name = 'OpenAI/gpt-4o-mini-transcribe'

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
openai_client = None
if OPENAI_CLIENT_AVAILABLE:
    openai_client = OpenAI(
        base_url=BASE_URL,
        api_key=API_KEY
    )

def get_audio_data(audio_path: str) -> Tuple[bytes, str]:
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯URLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Args:
        audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯URL
        
    Returns:
        (ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿, ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼)
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’å–å¾—
    file_format = audio_path.split('.')[-1].lower() if '.' in audio_path else 'mp3'
    
    # URLã®å ´åˆã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if audio_path.startswith(('http://', 'https://')):
        try:
            response = requests.get(audio_path)
            response.raise_for_status()
            return response.content, file_format
        except Exception as e:
            print(f"âŒ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            sys.exit(1)
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯èª­ã¿è¾¼ã¿
    else:
        try:
            with open(audio_path, 'rb') as f:
                return f.read(), file_format
        except Exception as e:
            print(f"âŒ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            sys.exit(1)

def process_audio_with_openai(audio_path: str, prompt: str = "What is in this recording?", model: str = model_name, language: str = None) -> str:
    """
    OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦éŸ³å£°å‡¦ç†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
    
    Args:
        audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯URL
        prompt: éŸ³å£°ã«é–¢ã™ã‚‹è³ªå•ã‚„æŒ‡ç¤ºï¼ˆchatå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        language: éŸ³å£°ã®è¨€èªï¼ˆçœç•¥å¯ï¼‰
        
    Returns:
        å‡¦ç†çµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
    """
    if not OPENAI_CLIENT_AVAILABLE or openai_client is None:
        print("âŒ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚requestsãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
        return process_audio_with_requests(audio_path, prompt, model, language)
    
    try:
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        audio_data, file_format = get_audio_data(audio_path)
        
        # chat completionsãƒ¢ãƒ‡ãƒ«ã¨transcriptionãƒ¢ãƒ‡ãƒ«ã‚’åŒºåˆ¥
        is_chat_model = model in ["gpt-4o-audio-preview", "OpenAI/gpt-4o-mini-transcribe", "SambaNova/Qwen2-Audio-7B-Instruct"]
        
        if is_chat_model:
            # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            encoded_audio = base64.b64encode(audio_data).decode('utf-8')
            
            # chat completionså½¢å¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            completion = openai_client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            { 
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "input_audio",
                                "input_audio": {
                                    "data": encoded_audio,
                                    "format": file_format
                                }
                            }
                        ]
                    },
                ]
            )
            
            result = completion.choices[0].message.content
            print(f"ğŸ“ å‡¦ç†çµæœ:\n{result}")
            return result
            
        else:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_format}') as temp_file:
                temp_file_path = temp_file.name
                temp_file.write(audio_data)
            
            try:
                # transcriptionå½¢å¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                with open(temp_file_path, "rb") as audio_file:
                    # languageå¼•æ•°ã®å‡¦ç†
                    kwargs = {}
                    if language:
                        kwargs["language"] = language
                    
                    response = openai_client.audio.transcriptions.create(
                        model=model,
                        file=audio_file,
                        **kwargs
                    )
                
                result = response.text
                print(f"ğŸ“ å‡¦ç†çµæœ:\n{result}")
                return result
                
            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                os.remove(temp_file_path)
        
    except Exception as e:
        print(f"âŒ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print("â†ªï¸ requestsãƒ¢ãƒ¼ãƒ‰ã§å†è©¦è¡Œã—ã¾ã™")
        return process_audio_with_requests(audio_path, prompt, model, language)

def process_audio_with_requests(audio_path: str, prompt: str = "What is in this recording?", model: str = model_name, language: str = None) -> str:
    """
    requestsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦éŸ³å£°å‡¦ç†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
    
    Args:
        audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯URL
        prompt: éŸ³å£°ã«é–¢ã™ã‚‹è³ªå•ã‚„æŒ‡ç¤ºï¼ˆchatå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        language: éŸ³å£°ã®è¨€èªï¼ˆçœç•¥å¯ï¼‰
        
    Returns:
        å‡¦ç†çµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        audio_data, file_format = get_audio_data(audio_path)
        
        # chat completionsãƒ¢ãƒ‡ãƒ«ã¨transcriptionãƒ¢ãƒ‡ãƒ«ã‚’åŒºåˆ¥
        is_chat_model = model in ["gpt-4o-audio-preview", "OpenAI/gpt-4o-mini-transcribe", "SambaNova/Qwen2-Audio-7B-Instruct"]
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = {
            "Content-Type": "application/json"
        }
        
        # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ 
        if API_KEY:
            headers["Authorization"] = f"Bearer {API_KEY}"
        
        if is_chat_model:
            # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            encoded_audio = base64.b64encode(audio_data).decode('utf-8')
            
            # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            endpoint = f"{BASE_URL}/chat/completions"
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæœ¬æ–‡
            payload = {
                "model": model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            { 
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "input_audio",
                                "input_audio": {
                                    "data": encoded_audio,
                                    "format": file_format
                                }
                            }
                        ]
                    }
                ]
            }
            
            # Geminiãƒ¢ãƒ‡ãƒ«ã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
            if "Google/gemini" in model:
                # Geminiã®å ´åˆã¯modalitiesãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œãªã„
                payload["modalities"] = ["text", "audio"]
            
            # APIå‘¼ã³å‡ºã—
            response = requests.post(endpoint, headers=headers, json=payload)
            response.raise_for_status()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = response.json()
            
            # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’æŠ½å‡º
            if "choices" in result and len(result["choices"]) > 0:
                message = result["choices"][0]["message"]
                if "content" in message:
                    text_response = message["content"]
                    print(f"ğŸ“ å‡¦ç†çµæœ:\n{text_response}")
                    return text_response
            
            print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {result}")
            return ""
            
        else:
            # transcriptionã®å ´åˆã¯ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦multipart/form-dataã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_format}') as temp_file:
                temp_file_path = temp_file.name
                temp_file.write(audio_data)
            
            try:
                # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
                endpoint = f"{BASE_URL}/audio/transcriptions"
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆmultipart/form-dataã®å ´åˆã¯Content-Typeã‚’å‰Šé™¤ï¼‰
                if "Content-Type" in headers:
                    del headers["Content-Type"]
                
                # ãƒ•ã‚©ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                files = {
                    'file': (os.path.basename(temp_file_path), open(temp_file_path, 'rb'), f'audio/{file_format}')
                }
                
                # ãƒ•ã‚©ãƒ¼ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æº–å‚™
                data = {
                    'model': model
                }
                
                # è¨€èªãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è¿½åŠ 
                if language:
                    data['language'] = language
                
                # APIå‘¼ã³å‡ºã—
                response = requests.post(endpoint, headers=headers, files=files, data=data)
                response.raise_for_status()
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
                result = response.json()
                
                # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’æŠ½å‡º
                if "text" in result:
                    text_response = result["text"]
                    print(f"ğŸ“ å‡¦ç†çµæœ:\n{text_response}")
                    return text_response
                
                print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {result}")
                return ""
                
            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                if os.path.exists(temp_file_path):
                    os.remove(temp_file_path)
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}")
        return ""

def process_audio(audio_path: str, prompt: str = "What is in this recording?", model: str = model_name, language: str = None, client_type: str = "auto") -> str:
    """
    éŸ³å£°å‡¦ç†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ï¼ˆçµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰
    
    Args:
        audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯URL
        prompt: éŸ³å£°ã«é–¢ã™ã‚‹è³ªå•ã‚„æŒ‡ç¤ºï¼ˆchatå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        language: éŸ³å£°ã®è¨€èªï¼ˆçœç•¥å¯ï¼‰
        client_type: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆopenai/requests/autoï¼‰
        
    Returns:
        å‡¦ç†çµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
    """
    print(f"ğŸµ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {audio_path}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model}")
    if language:
        print(f"ğŸŒ è¨€èª: {language}")
    print(f"ğŸ”§ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: {client_type}")
    
    # chatå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º
    if model in ["gpt-4o-audio-preview", "OpenAI/gpt-4o-mini-transcribe", "SambaNova/Qwen2-Audio-7B-Instruct"]:
        print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
    
    print("ğŸ”„ å‡¦ç†ä¸­...")
    
    if client_type == "openai" and OPENAI_CLIENT_AVAILABLE:
        return process_audio_with_openai(audio_path, prompt, model, language)
    elif client_type == "requests" or not OPENAI_CLIENT_AVAILABLE:
        return process_audio_with_requests(audio_path, prompt, model, language)
    else:  # auto
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ãã‚Œã‚’ä½¿ç”¨ã€ãã†ã§ãªã‘ã‚Œã°requests
        if OPENAI_CLIENT_AVAILABLE:
            return process_audio_with_openai(audio_path, prompt, model, language)
        else:
            return process_audio_with_requests(audio_path, prompt, model, language)

def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼šã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã¦æ©Ÿèƒ½ã‚’å®Ÿè¡Œ
    """
    parser = argparse.ArgumentParser(description='çµ±åˆéŸ³å£°å‡¦ç†ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ')
    parser.add_argument('audio_path', help='éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯URL')
    parser.add_argument('--prompt', '-p', default="What is in this recording?", help='éŸ³å£°ã«é–¢ã™ã‚‹è³ªå•ã‚„æŒ‡ç¤ºï¼ˆchatå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰')
    parser.add_argument('--model', '-m', default=model_name, help='ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å')
    parser.add_argument('--language', '-l', help='éŸ³å£°ã®è¨€èªï¼ˆä¾‹: ja, enï¼‰')
    parser.add_argument('--client', '-c', choices=['openai', 'requests', 'auto'], default='auto',
                       help='ä½¿ç”¨ã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆopenai/requests/autoï¼‰')
    
    args = parser.parse_args()
    
    process_audio(args.audio_path, args.prompt, args.model, args.language, args.client)

if __name__ == "__main__":
    main()